# Fiction_Eval
This is the Repo for the paper: "Towards A “Novel” Benchmark: Evaluating Literary Fiction with Large Language Models"
## Abstract
Current exploration on Large Language Models (LLMs) in creative generation focuses mainly on short stories, poetry, and scripts. With the expansion of Large Language Models (LLMs) context windows, "novel"\footnote{To clarify, the 3K–20K word fiction we study isn’t novel-length. We used “novel” in quotes to convey both its “fiction” and “new” meanings.} avenues emerge. This study aims to extend the boundaries of Natural Language Generation (NLG) evaluation by exploring LLMs' capabilities in more challenging long-form fiction. We propose a new multi-level evaluation framework that incorporates ten metrics across the Macro, Meso, and Micro levels. An annotated fiction dataset, sourced from human authors, LLMs, and human-AI collaborations in both English and Chinese is then constructed. Human evaluation reveals notable disparities between LLM-generated and human-authored fictions, particularly the “high-starting, low-ending” pattern in LLM outputs. We further probe ten high-performing LLMs through different prompt templates, achieving moderate correlations by strategically utilizing diverse LLMs tailored to different levels, as an initial step towards better automatic fiction evaluation. Finally, we offer a fine-grained analysis of LLMs capabilities through six issues, providing promising insights for future advancements.

## Contributions
\item We are the first to explore LLMs in long-form fiction evaluation and propose a multi-level framework for quantitative analysis;
\item We release an annotated fiction dataset with various sources in both English and Chinese, along with our refined guidelines, to benchmark literary fiction evaluation;
\item We evaluate ten top LLMs, initially enhancing automated fiction evaluation through utilizing different LLMs with diverse prompt strategies, and provide valuable insights to enhance their capabilities based on a fine-grained analysis.

## Citation
Please cite our work if you find it useful.
